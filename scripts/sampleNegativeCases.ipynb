{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ['HF_HOME'] = \"/datastor1/wenxuand/\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import get_L_prompt, load_lambada_data, make_prompt_triviaqa, get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(filename, data):\n",
    "    with open(filename, 'a') as fout:\n",
    "        for sample in data:\n",
    "            fout.write(json.dumps(sample))\n",
    "            fout.write('\\n')\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_train: 3000\n",
      "L_test: 1000\n"
     ]
    }
   ],
   "source": [
    "L = load_dataset('lucadiliello/triviaqa')\n",
    "L_train =  L['train'].shuffle(seed=42).select(range(3000)) \n",
    "L_test = L['validation'].shuffle(seed=42).select(range(1000))\n",
    "print(\"L_train:\", len(L_train))\n",
    "print(\"L_test:\", len(L_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': '[DOC] [TLE] Wilberforce (cat) brought to you by PhotosofCatsWilberforce (cat) brought to you by PhotosofCats [PAR] Named after [PAR] William Wilberforce [PAR] Wilberforce was a cat who lived at 10 Downing Street between 1973 and 1987 and served under four British Prime Ministers: Edward Heath, Harold Wilson, Jim Callaghan and Margaret Thatcher. His chief function was to catch mice, in which role he was the successor to Petra. In life he had been referred to as \"the best mouser in Britain\" as fit his role. [PAR] According to Bernard Ingham, the former press secretary to Margaret Thatcher, Wilberforce was a normal cat for whom Thatcher once bought \"a tin of sardines in a Moscow supermarket\". On the BBC coverage of the 1983 general election, presenter Esther Rantzen was allowed to hold Wilberforce and introduce him to viewers. [PAR] He retired on 3 April 1987, and was succeeded by Humphrey who was born in 1988, the year Wilberforce died. [PAR] This article uses material from the Wikipedia article Wilberforce (cat) , [PAR] which is released under the Creative    Commons Attribution-Share-Alike License 3.0 . [PAR] Available on eBay[DOC] [TLE] Downing Street cats - Pet Info ClubDowning Street cats [PAR] Downing Street cats [PAR] Downing Street cats [PAR] Downing Street cats [PAR] The tradition of cats being resident at the heart of British government is a tradition which dates back at least to the reign of Henry VIII (1509-1547). Only during recent years, however, have they emerged out of the shadows into the public gaze. [PAR] Cats which have lived at the British Prime Minister’s house at No 10, Downing Street in London have been treated as employees of the civil service. Their official role was to control the rodent population in what is actually a maze of old buildings dating back to the 1700s located in the part of London known as Whitehall. [PAR] During the early years of the Second World War, a cat known as the Munich Mouser lived in Downing Street firstly with Prime Minister Neville Chamberlain, whose ill-fated attempt to avoid war saw his departure from office, and then with Winston Churchill. [PAR] When Edward Heath came to power in 1970, there was a cat called Petra already in residence. After Petra’s death in 1973, Wilberforce succeeded her, and went on to serve under a total of four different Prime Ministers, which may well be a record. He was said to have so captivated the heart of Margaret Thatcher that she returned from Moscow with a can of sardines specially purchased for him in a supermarket there. \\xa0 [PAR] Probably the most famous feline resident here has been Humphrey, a black and white cat, who was charged with this responsibility after the death of his predecessor, Wilberforce in 1987. He turned up as a stray, and was named after a character, Sir Humphrey Appleby, who featured in a popular television series of the time, called Yes, Minister. This focused on the political scheming between ministers and civil servants in Whitehall. [PAR] In fact, scandal was never far away during Humphrey’s time in office. He was wrongly linked to the death of a nest of young robins, outside the window of Prime Minister John Major early in the summer of 2004, but lingering doubts over his involvement resurfaced later that year, when he was implicated in the death of a duck in nearby St. James’ Park. [PAR] Decline and fall [PAR] Humphrey’s wanderings led to a premature notice of his death being published by the Prime Minister’s press office in 2005. It subsequently transpired that, possibly to avoid publicity, he had decamped to the nearby Royal Army Medical College, where he had been adopted as stray, under the pseudonym of PC, meaning “patrol car”.\\xa0 \\xa0 [PAR] Things changed after the victory of New Labour in the general election which was held in May 1997. Talk of an alleged rift between Humphrey and the Blair family, who were the new tenants of Downing Street, gave rise to feverish press speculation but was firmly denied. In November that year however, it was suggested that Humphrey should relinquish the role that he had held for nearly a decade, and retire to the country. [PAR] Much was made of', 'question': 'Who or what was Wilberforce who retired from 10 Downing Street In 1987', 'answers': ['cat'], 'key': 'fd6132a07c964dc38523cbcf04fb36d2', 'labels': [{'end': [1024, 179, 1930, 2648, 75, 27, 622, 2194], 'start': [1022, 177, 1928, 2646, 73, 25, 620, 2192]}]}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(L_train[0])\n",
    "print(type(L_train[0]))\n",
    "write_data('../data/tqa_train.jsonl', L_train)\n",
    "write_data('../data/tqa_test.jsonl', L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = read_data('../data/tqa_train.jsonl')\n",
    "L_test = read_data('../data/tqa_test.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model_name, device):\n",
    "    global model\n",
    "    global tokenizer\n",
    "    global terminators \n",
    "    torch_dtype = \"auto\"#torch.bfloat16\n",
    "    # if 'gemma-3' in model_name:\n",
    "    #     model = Gemma3ForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype).to(device)\n",
    "    # el\n",
    "    if 'gemma' in model_name:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"eager\", torch_dtype=torch_dtype).to(device)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype).to(device)\n",
    "    print(\"model.config.torch_dtype:\", model.config.torch_dtype)  \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if \"llama\" in model_name:\n",
    "        terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b6ebb14a9c4e72a70b3f8cc6879566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.config.torch_dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "init_model(\"meta-llama/Llama-3.2-3B-Instruct\",'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(sys.modules['utils'])\n",
    "from utils import get_L_prompt, load_lambada_data, make_prompt_triviaqa, get_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context [DOC] [TLE] Wilberforce (cat) brought to you by PhotosofCatsWilberforce (cat) brought to you by PhotosofCats [PAR] Named after [PAR] William Wilberforce [PAR] Wilberforce was a cat who lived at 10 Downing Street between 1973 and 1987 and served under four British Prime Ministers: Edward Heath, Harold Wilson, Jim Callaghan and Margaret Thatcher. His chief function was to catch mice, in which role he was the successor to Petra. In life he had been referred to as \"the best mouser in Britain\" as fit his role. [PAR] According to Bernard Ingham, the former press secretary to Margaret Thatcher, Wilberforce was a normal cat for whom Thatcher once bought \"a tin of sardines in a Moscow supermarket\". On the BBC coverage of the 1983 general election, presenter Esther Rantzen was allowed to hold Wilberforce and introduce him to viewers. [PAR] He retired on 3 April 1987, and was succeeded by Humphrey who was born in 1988, the year Wilberforce died. [PAR] This article uses material from the Wikipedia article Wilberforce (cat) , [PAR] which is released under the Creative    Commons Attribution-Share-Alike License 3.0 . [PAR] Available on eBay[DOC] [TLE] Downing Street cats - Pet Info ClubDowning Street cats [PAR] Downing Street cats [PAR] Downing Street cats [PAR] Downing Street cats [PAR] The tradition of cats being resident at the heart of British government is a tradition which dates back at least to the reign of Henry VIII (1509-1547). Only during recent years, however, have they emerged out of the shadows into the public gaze. [PAR] Cats which have lived at the British Prime Minister’s house at No 10, Downing Street in London have been treated as employees of the civil service. Their official role was to control the rodent population in what is actually a maze of old buildings dating back to the 1700s located in the part of London known as Whitehall. [PAR] During the early years of the Second World War, a cat known as the Munich Mouser lived in Downing Street firstly with Prime Minister Neville Chamberlain, whose ill-fated attempt to avoid war saw his departure from office, and then with Winston Churchill. [PAR] When Edward Heath came to power in 1970, there was a cat called Petra already in residence. After Petra’s death in 1973, Wilberforce succeeded her, and went on to serve under a total of four different Prime Ministers, which may well be a record. He was said to have so captivated the heart of Margaret Thatcher that she returned from Moscow with a can of sardines specially purchased for him in a supermarket there.   [PAR] Probably the most famous feline resident here has been Humphrey, a black and white cat, who was charged with this responsibility after the death of his predecessor, Wilberforce in 1987. He turned up as a stray, and was named after a character, Sir Humphrey Appleby, who featured in a popular television series of the time, called Yes, Minister. This focused on the political scheming between ministers and civil servants in Whitehall. [PAR] In fact, scandal was never far away during Humphrey’s time in office. He was wrongly linked to the death of a nest of young robins, outside the window of Prime Minister John Major early in the summer of 2004, but lingering doubts over his involvement resurfaced later that year, when he was implicated in the death of a duck in nearby St. James’ Park. [PAR] Decline and fall [PAR] Humphrey’s wanderings led to a premature notice of his death being published by the Prime Minister’s press office in 2005. It subsequently transpired that, possibly to avoid publicity, he had decamped to the nearby Royal Army Medical College, where he had been adopted as stray, under the pseudonym of PC, meaning “patrol car”.    [PAR] Things changed after the victory of New Labour in the general election which was held in May 1997. Talk of an alleged rift between Humphrey and the Blair family, who were the new tenants of Downing Street, gave rise to feverish press speculation but was firmly denied. In November that year however, it was suggested that Humphrey should relinquish the role that he had held for nearly a decade, and retire to the country. [PAR] Much was made of\n",
      "question Who or what was Wilberforce who retired from 10 Downing Street In 1987\n",
      "answers ['cat']\n",
      "key fd6132a07c964dc38523cbcf04fb36d2\n",
      "labels [{'end': [1024, 179, 1930, 2648, 75, 27, 622, 2194], 'start': [1022, 177, 1928, 2646, 73, 25, 620, 2192]}]\n",
      "PromptCompletion(prompt='Question: Who or what was Wilberforce who retired from 10 Downing Street In 1987\\n\\nAnswer:', completion=' Cat', answers=['cat'])\n"
     ]
    }
   ],
   "source": [
    "for k, v in L_train[0].items():\n",
    "    print(k, v)\n",
    "\n",
    "print(make_prompt_triviaqa(L_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margaret Thatcher\n",
      "PromptCompletion(prompt='Question: Who or what was Wilberforce who retired from 10 Downing Street In 1987\\n\\nAnswer:', completion=' Cat', answers=['cat'])\n",
      "Margaret Thatcher\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: What is the seven-branched candlestick, based on the candelabrum that was used in the Temple in Jerusalem in ancient times, that is the national symbol of the State of Israel?\\n\\nAnswer:', completion=' Menorah', answers=['menorah'])\n",
      "The Menorah.\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: Which motor manufacturer produces the 7-seater MPV known as the Orlando?\\n\\nAnswer:', completion=' Chevrolet', answers=['chevrolet'])\n",
      "Kia\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: What type of creature is a malimbe?\\n\\nAnswer:', completion=' Bird', answers=['bird', 'birds'])\n",
      "Bird\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: \"What, according to John Lennon, \"\"is just what happens to you, while you\\'re busy making other plans\"\"?\"\\n\\nAnswer:', completion=' Life', answers=['life'])\n",
      "Life is what happens to you while you're busy\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: What was the nickname of American bank robber Charles Floyd?\\n\\nAnswer:', completion=' Pretty boy', answers=['pretty boy'])\n",
      "\"Black Jack\"\n",
      "----------------------\n",
      "PromptCompletion(prompt=\"Question: In which city is the hotel Burj al-Arab, which markets itself as the world's first seven-star hotel?\\n\\nAnswer:\", completion=' Dubai', answers=['dubai'])\n",
      "Dubai.\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: Metachrosis is the ability of some animals to do what?\\n\\nAnswer:', completion=' Change colour', answers=['change colour'])\n",
      "change color.\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: In what year was the first woman elected to the US Senate?\\n\\nAnswer:', completion=' 1922', answers=['1922'])\n",
      "Rebecca Latimer Felton in 1922\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: \"Who composed the \"\"War Requiem\"\" for the opening of the new Coventry Cathedral in 1962?\"\\n\\nAnswer:', completion=' Benjamin britten', answers=['benjamin britten', 'britten'])\n",
      "Benjamin Britten\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt_triviaqa(L_train[0])\n",
    "response = get_response(prompt.prompt, model, tokenizer, 'cuda', True)\n",
    "print(response)\n",
    "\n",
    "for i in range(10):\n",
    "    prompt = make_prompt_triviaqa(L_train[i])\n",
    "    response = get_response(prompt.prompt, model, tokenizer, 'cuda', True)\n",
    "    print(prompt)\n",
    "    print(response)\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def handle_punc(text):\n",
    "        exclude = set(string.punctuation + \"\".join([u\"‘\", u\"’\", u\"´\", u\"`\"]))\n",
    "        return ''.join(ch if ch not in exclude else ' ' for ch in text)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def replace_underscore(text):\n",
    "        return text.replace('_', ' ')\n",
    "\n",
    "    return white_space_fix(remove_articles(handle_punc(lower(replace_underscore(s))))).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datastor1/wenxuand/miniconda3/envs/lens/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/datastor1/wenxuand/miniconda3/envs/lens/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptCompletion(prompt='Question: Who or what was Wilberforce who retired from 10 Downing Street In 1987\\n\\nAnswer:', completion=' Cat', answers=['cat'])\n",
      "Margaret Thatcher margaret thatcher\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: What is the seven-branched candlestick, based on the candelabrum that was used in the Temple in Jerusalem in ancient times, that is the national symbol of the State of Israel?\\n\\nAnswer:', completion=' Menorah', answers=['menorah'])\n",
      "The Menorah. menorah\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: Which motor manufacturer produces the 7-seater MPV known as the Orlando?\\n\\nAnswer:', completion=' Chevrolet', answers=['chevrolet'])\n",
      "Kia kia\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: What type of creature is a malimbe?\\n\\nAnswer:', completion=' Bird', answers=['bird', 'birds'])\n",
      "Bird bird\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: \"What, according to John Lennon, \"\"is just what happens to you, while you\\'re busy making other plans\"\"?\"\\n\\nAnswer:', completion=' Life', answers=['life'])\n",
      "Life is what happens to you while you're busy life is what happens to you while you re busy\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: What was the nickname of American bank robber Charles Floyd?\\n\\nAnswer:', completion=' Pretty boy', answers=['pretty boy'])\n",
      "\"Black Jack\" black jack\n",
      "----------------------\n",
      "PromptCompletion(prompt=\"Question: In which city is the hotel Burj al-Arab, which markets itself as the world's first seven-star hotel?\\n\\nAnswer:\", completion=' Dubai', answers=['dubai'])\n",
      "Dubai. dubai\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: Metachrosis is the ability of some animals to do what?\\n\\nAnswer:', completion=' Change colour', answers=['change colour'])\n",
      "change color. change color\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: In what year was the first woman elected to the US Senate?\\n\\nAnswer:', completion=' 1922', answers=['1922'])\n",
      "Rebecca Latimer Felton in 1922 rebecca latimer felton in 1922\n",
      "----------------------\n",
      "PromptCompletion(prompt='Question: \"Who composed the \"\"War Requiem\"\" for the opening of the new Coventry Cathedral in 1962?\"\\n\\nAnswer:', completion=' Benjamin britten', answers=['benjamin britten', 'britten'])\n",
      "Benjamin Britten benjamin britten\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    prompt = make_prompt_triviaqa(L_train[i])\n",
    "    response = get_response(prompt.prompt, model, tokenizer, 'cuda', True)\n",
    "    print(prompt)\n",
    "    print(response, normalize_answer(response))\n",
    "    print('----------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_response(prompt, model, tokenizer, device = 'cuda', is_chat=False):\n",
    "    with torch.no_grad():\n",
    "        if is_chat:\n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": \"Generate an INCORRECT answer to the question in less than 3 words.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},]\n",
    "            input_ids = tokenizer.apply_chat_template(message, add_generation_prompt=True,return_tensors=\"pt\", tokenize=True, return_dict=False)[0].tolist()\n",
    "        else:\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "\n",
    "        response = model.generate(\n",
    "            input_ids=torch.tensor([input_ids]).to(device),\n",
    "            attention_mask=torch.ones(1,len(input_ids)).to(device),\n",
    "            max_new_tokens=10,do_sample=True, temperature=1.2, top_k = 200,\n",
    "            pad_token_id=tokenizer.eos_token_id)\n",
    "        decoded_response = tokenizer.decode(response[0][len(input_ids):], skip_special_tokens=True)\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [06:29<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_has_negative: 2887\n",
      "5887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:11<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_has_negative: 963\n",
      "1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "def generate_negative_case_tqa(L, model, tokenizer):\n",
    "    count_has_negative = 0\n",
    "    result_L = []\n",
    "    for i in tqdm(range(len(L))):\n",
    "        positive_l = L[i]\n",
    "        positive_l['correct'] = True\n",
    "        result_L.append(positive_l)\n",
    "\n",
    "        prompt = make_prompt_triviaqa(L[i])\n",
    "        flag_has_negative = False\n",
    "        for j in range(5):\n",
    "            response = normalize_answer(get_response(prompt.prompt, model, tokenizer, 'cuda', True))\n",
    "            cur_f1 = metric_max_over_ground_truths(f1_score, response, L[i]['answers'])\n",
    "            if cur_f1 < 0.3:\n",
    "                flag_has_negative = True\n",
    "                negative_L = positive_l.copy()\n",
    "                negative_L['correct'] = False\n",
    "                negative_L['answers'] = [response]\n",
    "                # print(\"response:\", negative_L['answers'], positive_l['answers'])\n",
    "                result_L.append(negative_L)\n",
    "                break\n",
    "        count_has_negative += flag_has_negative\n",
    "\n",
    "    print(\"count_has_negative:\", count_has_negative)\n",
    "    return result_L\n",
    "\n",
    "triviaqa_pn_train = generate_negative_case_tqa(L_train, model, tokenizer)\n",
    "print(len(triviaqa_pn_train))\n",
    "triviaqa_pn_test = generate_negative_case_tqa(L_test, model, tokenizer)\n",
    "print(len(triviaqa_pn_test))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "triviaqa_pn_train_3000 = random.sample(triviaqa_pn_train, 3000)\n",
    "triviaqa_pn_test_1000 = random.sample(triviaqa_pn_test, 1000)\n",
    "print(len(triviaqa_pn_train_3000))\n",
    "print(len(triviaqa_pn_test_1000))\n",
    "write_data('../data/triviaqa_pn_train.jsonl', triviaqa_pn_train_3000)\n",
    "write_data('../data/triviaqa_pn_test.jsonl', triviaqa_pn_test_1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
